"""
å¹¶è¡Œæ‰«ææ‰§è¡Œå™¨
æ”¯æŒå¤šå·¥å…·å¹¶è¡Œæ‰§è¡Œï¼Œæé«˜æ‰«ææ•ˆç‡
"""

import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FutureTimeoutError
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class ScanTask:
    """æ‰«æä»»åŠ¡æ•°æ®ç±»"""
    tool_name: str
    target: str
    params: Dict[str, Any] = field(default_factory=dict)
    timeout: int = 300
    priority: int = 0  # ä¼˜å…ˆçº§ï¼šæ•°å€¼è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜


@dataclass
class ScanResult:
    """æ‰«æç»“æœæ•°æ®ç±»"""
    tool_name: str
    target: str
    success: bool
    result: Dict[str, Any]
    execution_time: float
    error: Optional[str] = None
    timed_out: bool = False


class ParallelScanner:
    """å¹¶è¡Œæ‰«ææ‰§è¡Œå™¨"""
    
    # å·¥å…·é»˜è®¤è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    DEFAULT_TIMEOUTS = {
        'httpx': 30,
        'nuclei': 300,
        'nmap': 120,
        'nmap-advanced': 300,
        'sqlmap': 600,
        'nikto': 300,
        'gobuster': 180,
        'feroxbuster': 180,
        'ffuf': 180,
        'amass': 600,
        'subfinder': 60,
        'katana': 120,
        'dalfox': 300,
        'arjun': 120,
        'masscan': 180,
    }
    
    def __init__(self, max_workers: int = 5):
        """
        åˆå§‹åŒ–å¹¶è¡Œæ‰«æå™¨
        
        Args:
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        logger.info(f"ğŸš€ Parallel scanner initialized with {max_workers} workers")
    
    def execute_single_task(
        self, 
        task: ScanTask, 
        executor_func: Callable
    ) -> ScanResult:
        """
        æ‰§è¡Œå•ä¸ªæ‰«æä»»åŠ¡
        
        Args:
            task: æ‰«æä»»åŠ¡
            executor_func: å·¥å…·æ‰§è¡Œå‡½æ•°
            
        Returns:
            ScanResult: æ‰«æç»“æœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ”§ Executing {task.tool_name} on {task.target}")
            
            # æ‰§è¡Œå·¥å…·
            result = executor_func(task.target, task.params)
            
            execution_time = time.time() - start_time
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            success = result.get('success', False) if isinstance(result, dict) else False
            
            return ScanResult(
                tool_name=task.tool_name,
                target=task.target,
                success=success,
                result=result,
                execution_time=execution_time,
                timed_out=result.get('timed_out', False) if isinstance(result, dict) else False
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ {task.tool_name} failed: {str(e)}")
            
            return ScanResult(
                tool_name=task.tool_name,
                target=task.target,
                success=False,
                result={},
                execution_time=execution_time,
                error=str(e)
            )
    
    def execute_parallel(
        self, 
        tasks: List[ScanTask],
        tool_executors: Dict[str, Callable],
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, ScanResult]:
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ‰«æä»»åŠ¡
        
        Args:
            tasks: æ‰«æä»»åŠ¡åˆ—è¡¨
            tool_executors: å·¥å…·æ‰§è¡Œå™¨å­—å…¸ {tool_name: executor_func}
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°(completed, total, current_tool)
            
        Returns:
            Dict[str, ScanResult]: å·¥å…·åç§°åˆ°æ‰«æç»“æœçš„æ˜ å°„
        """
        if not tasks:
            logger.warning("âš ï¸  No tasks to execute")
            return {}
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡
        sorted_tasks = sorted(tasks, key=lambda t: t.priority, reverse=True)
        
        results = {}
        total_tasks = len(sorted_tasks)
        completed_tasks = 0
        
        logger.info(f"ğŸš€ Starting parallel execution of {total_tasks} tasks")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {}
            
            for task in sorted_tasks:
                # è·å–å·¥å…·æ‰§è¡Œå™¨
                executor_func = tool_executors.get(task.tool_name)
                
                if not executor_func:
                    logger.error(f"âŒ No executor found for {task.tool_name}")
                    results[task.tool_name] = ScanResult(
                        tool_name=task.tool_name,
                        target=task.target,
                        success=False,
                        result={},
                        execution_time=0,
                        error=f"No executor found for {task.tool_name}"
                    )
                    continue
                
                # æäº¤ä»»åŠ¡
                future = executor.submit(
                    self.execute_single_task,
                    task,
                    executor_func
                )
                future_to_task[future] = task
            
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            for future in as_completed(future_to_task, timeout=None):
                task = future_to_task[future]
                
                try:
                    # è·å–ç»“æœï¼ˆå¸¦è¶…æ—¶ï¼‰
                    result = future.result(timeout=task.timeout)
                    results[task.tool_name] = result
                    
                    completed_tasks += 1
                    
                    if result.success:
                        logger.info(
                            f"âœ… [{completed_tasks}/{total_tasks}] "
                            f"{task.tool_name} completed in {result.execution_time:.2f}s"
                        )
                    else:
                        logger.warning(
                            f"âš ï¸  [{completed_tasks}/{total_tasks}] "
                            f"{task.tool_name} failed"
                        )
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress_callback(completed_tasks, total_tasks, task.tool_name)
                    
                except FutureTimeoutError:
                    logger.error(f"â±ï¸  {task.tool_name} timed out after {task.timeout}s")
                    results[task.tool_name] = ScanResult(
                        tool_name=task.tool_name,
                        target=task.target,
                        success=False,
                        result={},
                        execution_time=task.timeout,
                        error=f"Timeout after {task.timeout}s",
                        timed_out=True
                    )
                    completed_tasks += 1
                    
                except Exception as e:
                    logger.error(f"âŒ {task.tool_name} raised exception: {str(e)}")
                    results[task.tool_name] = ScanResult(
                        tool_name=task.tool_name,
                        target=task.target,
                        success=False,
                        result={},
                        execution_time=0,
                        error=str(e)
                    )
                    completed_tasks += 1
        
        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        successful = sum(1 for r in results.values() if r.success)
        failed = len(results) - successful
        total_time = sum(r.execution_time for r in results.values())
        
        logger.info(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¯ Parallel Scan Execution Summary        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Tasks:     {total_tasks:4d}                    â”‚
â”‚ Successful:      {successful:4d} âœ…                  â”‚
â”‚ Failed:          {failed:4d} âŒ                  â”‚
â”‚ Total Time:      {total_time:6.2f}s                â”‚
â”‚ Avg Time/Task:   {total_time/total_tasks if total_tasks > 0 else 0:6.2f}s                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        return results
    
    def get_default_timeout(self, tool_name: str) -> int:
        """
        è·å–å·¥å…·çš„é»˜è®¤è¶…æ—¶æ—¶é—´
        
        Args:
            tool_name: å·¥å…·åç§°
            
        Returns:
            int: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        return self.DEFAULT_TIMEOUTS.get(tool_name, 300)
    
    def create_task_from_selection(
        self, 
        tool_name: str, 
        target: str, 
        params: Optional[Dict] = None,
        priority: int = 0
    ) -> ScanTask:
        """
        ä»å·¥å…·é€‰æ‹©åˆ›å»ºæ‰«æä»»åŠ¡
        
        Args:
            tool_name: å·¥å…·åç§°
            target: ç›®æ ‡
            params: å‚æ•°
            priority: ä¼˜å…ˆçº§
            
        Returns:
            ScanTask: æ‰«æä»»åŠ¡
        """
        return ScanTask(
            tool_name=tool_name,
            target=target,
            params=params or {},
            timeout=self.get_default_timeout(tool_name),
            priority=priority
        )


class SmartParallelScanner(ParallelScanner):
    """æ™ºèƒ½å¹¶è¡Œæ‰«æå™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, max_workers: int = 5):
        super().__init__(max_workers)
        self.execution_history = []
    
    def execute_with_retry(
        self,
        tasks: List[ScanTask],
        tool_executors: Dict[str, Callable],
        max_retries: int = 2,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, ScanResult]:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„å¹¶è¡Œæ‰§è¡Œ
        
        Args:
            tasks: æ‰«æä»»åŠ¡åˆ—è¡¨
            tool_executors: å·¥å…·æ‰§è¡Œå™¨å­—å…¸
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            Dict[str, ScanResult]: æ‰«æç»“æœ
        """
        all_results = {}
        remaining_tasks = tasks.copy()
        retry_count = 0
        
        while remaining_tasks and retry_count <= max_retries:
            if retry_count > 0:
                logger.info(f"ğŸ”„ Retry attempt {retry_count}/{max_retries} for {len(remaining_tasks)} tasks")
            
            # æ‰§è¡Œå½“å‰æ‰¹æ¬¡
            results = self.execute_parallel(
                remaining_tasks,
                tool_executors,
                progress_callback
            )
            
            # æ›´æ–°æ€»ç»“æœ
            all_results.update(results)
            
            # æ‰¾å‡ºå¤±è´¥çš„ä»»åŠ¡ï¼ˆéè¶…æ—¶ï¼‰
            failed_tasks = [
                task for task in remaining_tasks
                if task.tool_name in results 
                and not results[task.tool_name].success
                and not results[task.tool_name].timed_out
            ]
            
            if not failed_tasks or retry_count >= max_retries:
                break
            
            remaining_tasks = failed_tasks
            retry_count += 1
            
            # çŸ­æš‚å»¶è¿Ÿåé‡è¯•
            time.sleep(2)
        
        return all_results
    
    def execute_with_dependencies(
        self,
        task_groups: List[List[ScanTask]],
        tool_executors: Dict[str, Callable],
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, ScanResult]:
        """
        æŒ‰ä¾èµ–é¡ºåºæ‰§è¡Œä»»åŠ¡ç»„
        æ¯ç»„å†…å¹¶è¡Œæ‰§è¡Œï¼Œç»„é—´ä¸²è¡Œæ‰§è¡Œ
        
        Args:
            task_groups: ä»»åŠ¡ç»„åˆ—è¡¨ï¼ŒæŒ‰ä¾èµ–é¡ºåºæ’åˆ—
            tool_executors: å·¥å…·æ‰§è¡Œå™¨å­—å…¸
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            Dict[str, ScanResult]: æ‰€æœ‰æ‰«æç»“æœ
        """
        all_results = {}
        
        for group_idx, task_group in enumerate(task_groups, 1):
            logger.info(f"ğŸ“‹ Executing task group {group_idx}/{len(task_groups)}")
            
            # å¹¶è¡Œæ‰§è¡Œå½“å‰ç»„
            group_results = self.execute_parallel(
                task_group,
                tool_executors,
                progress_callback
            )
            
            all_results.update(group_results)
            
            # æ£€æŸ¥ç»„å†…æ˜¯å¦æœ‰å…³é”®ä»»åŠ¡å¤±è´¥
            critical_failures = [
                task.tool_name for task in task_group
                if not group_results.get(task.tool_name, ScanResult(
                    tool_name=task.tool_name,
                    target=task.target,
                    success=False,
                    result={},
                    execution_time=0
                )).success
            ]
            
            if critical_failures:
                logger.warning(
                    f"âš ï¸  Group {group_idx} has failed tasks: {', '.join(critical_failures)}"
                )
        
        return all_results


# å…¨å±€å®ä¾‹
parallel_scanner = ParallelScanner(max_workers=5)
smart_scanner = SmartParallelScanner(max_workers=5)
