"""
å¢å¼ºçš„æ™ºèƒ½æ‰«æè·¯ç”±
é›†æˆäº†å·¥å…·æ£€æŸ¥ã€å¹¶è¡Œæ‰§è¡Œã€ç¼“å­˜ä¼˜åŒ–å’Œé”™è¯¯å¤„ç†
"""

import logging
from datetime import datetime
from flask import Blueprint, request, jsonify

# å¯¼å…¥æ–°æ¨¡å—
from core.utils.tool_checker import tool_checker
from core.execution.parallel_scanner import ParallelScanner, ScanTask
from core.cache.scan_cache import cache_executor
from core.execution.error_handler import resilient_executor

logger = logging.getLogger(__name__)

# Create blueprint
intelligence_enhanced_bp = Blueprint('intelligence_enhanced', __name__, url_prefix='/api/intelligence/v2')

# Dependencies
decision_engine = None
tool_executors = None


def init_app(dec_engine, executors):
    """Initialize blueprint with dependencies"""
    global decision_engine, tool_executors
    decision_engine = dec_engine
    tool_executors = executors


@intelligence_enhanced_bp.route("/tool-check", methods=["GET"])
def check_tools():
    """æ£€æŸ¥ç³»ç»Ÿå·¥å…·å¯ç”¨æ€§"""
    try:
        logger.info("ğŸ” Checking system tools availability")
        
        report = tool_checker.get_system_report()
        
        logger.info(
            f"âœ… Tool check completed: "
            f"{report['available_tools']}/{report['total_tools']} available "
            f"({report['coverage_percentage']:.1f}%)"
        )
        
        return jsonify({
            "success": True,
            "report": report,
            "timestamp": datetime.now().isoformat()
        })
    
    except Exception as e:
        logger.error(f"âŒ Tool check error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@intelligence_enhanced_bp.route("/generate-install-script", methods=["POST"])
def generate_install_script():
    """ç”Ÿæˆå·¥å…·å®‰è£…è„šæœ¬"""
    try:
        data = request.get_json() or {}
        output_file = data.get('output_file', 'install_missing_tools.sh')
        
        logger.info(f"ğŸ“ Generating install script: {output_file}")
        
        script_path = tool_checker.generate_install_script(output_file)
        
        if script_path:
            return jsonify({
                "success": True,
                "script_path": script_path,
                "message": f"Install script generated: {script_path}",
                "usage": f"Run with: ./{script_path}"
            })
        else:
            return jsonify({
                "success": True,
                "message": "All tools are already installed!"
            })
    
    except Exception as e:
        logger.error(f"âŒ Script generation error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@intelligence_enhanced_bp.route("/smart-scan-enhanced", methods=["POST"])
def smart_scan_enhanced():
    """
    å¢å¼ºçš„æ™ºèƒ½æ‰«æ
    é›†æˆå·¥å…·æ£€æŸ¥ã€å¹¶è¡Œæ‰§è¡Œã€ç¼“å­˜å’Œé”™è¯¯å¤„ç†
    """
    try:
        data = request.get_json()
        if not data or 'target' not in data:
            return jsonify({"error": "Target is required"}), 400
        
        target = data['target']
        objective = data.get('objective', 'comprehensive')
        max_tools = data.get('max_tools', 5)
        force_refresh = data.get('force_refresh', False)
        max_workers = data.get('max_workers', 5)
        enable_cache = data.get('enable_cache', True)
        enable_retry = data.get('enable_retry', True)
        enable_fallback = data.get('enable_fallback', True)
        
        logger.info(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ ğŸš€ ENHANCED INTELLIGENT SCAN                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Target:          {target:<44} â•‘
â•‘ Objective:       {objective:<44} â•‘
â•‘ Max Tools:       {max_tools:<44} â•‘
â•‘ Cache:           {'Enabled' if enable_cache else 'Disabled':<44} â•‘
â•‘ Retry:           {'Enabled' if enable_retry else 'Disabled':<44} â•‘
â•‘ Fallback:        {'Enabled' if enable_fallback else 'Disabled':<44} â•‘
â•‘ Max Workers:     {max_workers:<44} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        # 1. åˆ†æç›®æ ‡
        logger.info("ğŸ“Š Step 1/5: Analyzing target...")
        profile = decision_engine.analyze_target(target)
        
        # 2. é€‰æ‹©æœ€ä¼˜å·¥å…·
        logger.info("ğŸ¯ Step 2/5: Selecting optimal tools...")
        selected_tools = decision_engine.select_optimal_tools(profile, objective)[:max_tools]
        
        # 3. è¿‡æ»¤å¯ç”¨å·¥å…·
        logger.info("ğŸ” Step 3/5: Checking tool availability...")
        available_tools = []
        unavailable_tools = []
        
        for tool in selected_tools:
            if tool_checker.is_tool_available(tool):
                available_tools.append(tool)
            else:
                unavailable_tools.append(tool)
                logger.warning(f"âš ï¸  Tool not available: {tool}")
        
        if not available_tools:
            return jsonify({
                "success": False,
                "error": "No available tools found",
                "unavailable_tools": unavailable_tools,
                "suggestions": [
                    tool_checker.check_tool_or_error(tool)
                    for tool in unavailable_tools
                ]
            }), 400
        
        logger.info(f"âœ… Available tools: {len(available_tools)}/{len(selected_tools)}")
        
        # 4. åˆ›å»ºæ‰«æä»»åŠ¡
        logger.info("ğŸ“‹ Step 4/5: Creating scan tasks...")
        scanner = ParallelScanner(max_workers=max_workers)
        tasks = []
        
        for tool_name in available_tools:
            # è·å–ä¼˜åŒ–çš„å‚æ•°
            optimized_params = decision_engine.optimize_parameters(tool_name, profile)
            
            # åˆ›å»ºä»»åŠ¡
            task = scanner.create_task_from_selection(
                tool_name=tool_name,
                target=target,
                params=optimized_params,
                priority=1 if 'nmap' in tool_name or 'nuclei' in tool_name else 0
            )
            tasks.append(task)
        
        # 5. æ‰§è¡Œæ‰«æï¼ˆå¸¦ç¼“å­˜å’Œé”™è¯¯å¤„ç†ï¼‰
        logger.info("ğŸš€ Step 5/5: Executing parallel scan...")
        
        def create_cached_executor(tool_name):
            """ä¸ºå·¥å…·åˆ›å»ºå¸¦ç¼“å­˜çš„æ‰§è¡Œå™¨"""
            original_executor = tool_executors.get(tool_name)
            
            if not original_executor:
                return None
            
            def cached_wrapper(target, params):
                if enable_cache and not force_refresh:
                    # å°è¯•ä»ç¼“å­˜è·å–
                    return cache_executor.execute_with_cache(
                        tool_name=tool_name,
                        target=target,
                        params=params,
                        executor_func=original_executor,
                        force_refresh=force_refresh,
                        scan_type=objective
                    )
                else:
                    # ç›´æ¥æ‰§è¡Œ
                    return original_executor(target, params)
            
            # å¦‚æœå¯ç”¨äº†é‡è¯•å’Œå›é€€
            if enable_retry or enable_fallback:
                def resilient_wrapper(target, params):
                    return resilient_executor.execute_with_resilience(
                        tool_name=tool_name,
                        target=target,
                        params=params,
                        executor_func=cached_wrapper,
                        tool_executors=tool_executors
                    )
                return resilient_wrapper
            else:
                return cached_wrapper
        
        # åˆ›å»ºå¢å¼ºçš„æ‰§è¡Œå™¨å­—å…¸
        enhanced_executors = {
            tool_name: create_cached_executor(tool_name)
            for tool_name in available_tools
        }
        
        # è¿›åº¦å›è°ƒ
        progress = {"completed": 0, "total": len(tasks)}
        
        def progress_callback(completed, total, current_tool):
            progress["completed"] = completed
            logger.info(f"ğŸ“Š Progress: {completed}/{total} ({completed/total*100:.1f}%) - Completed: {current_tool}")
        
        # æ‰§è¡Œå¹¶è¡Œæ‰«æ
        results = scanner.execute_parallel(
            tasks=tasks,
            tool_executors=enhanced_executors,
            progress_callback=progress_callback
        )
        
        # 6. å¤„ç†ç»“æœ
        logger.info("ğŸ“Š Processing results...")
        
        tools_executed = []
        total_vulnerabilities = 0
        successful_tools = []
        failed_tools = []
        cached_results = 0
        
        for tool_name, scan_result in results.items():
            tool_data = {
                "tool": tool_name,
                "success": scan_result.success,
                "execution_time": scan_result.execution_time,
                "timed_out": scan_result.timed_out,
                "error": scan_result.error,
                "from_cache": scan_result.result.get('from_cache', False),
                "used_alternative": scan_result.result.get('used_alternative', False),
                "result": scan_result.result
            }
            
            tools_executed.append(tool_data)
            
            if scan_result.success:
                successful_tools.append(tool_name)
                
                # ç»Ÿè®¡æ¼æ´
                if 'stdout' in scan_result.result:
                    output = scan_result.result['stdout']
                    vuln_indicators = ['CRITICAL', 'HIGH', 'MEDIUM', 'VULNERABILITY', 'SQL injection', 'XSS']
                    vuln_count = sum(1 for indicator in vuln_indicators if indicator.lower() in output.lower())
                    total_vulnerabilities += vuln_count
            else:
                failed_tools.append(tool_name)
            
            if tool_data.get('from_cache'):
                cached_results += 1
        
        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        total_time = sum(r.execution_time for r in results.values())
        
        response = {
            "success": True,
            "target": target,
            "objective": objective,
            "target_profile": profile.to_dict(),
            "tools_executed": tools_executed,
            "execution_summary": {
                "total_tools": len(tasks),
                "successful_tools": len(successful_tools),
                "failed_tools": len(failed_tools),
                "cached_results": cached_results,
                "unavailable_tools": len(unavailable_tools),
                "total_execution_time": round(total_time, 2),
                "average_time_per_tool": round(total_time / len(tasks), 2) if tasks else 0,
                "successful_tool_names": successful_tools,
                "failed_tool_names": failed_tools,
                "unavailable_tool_names": unavailable_tools
            },
            "vulnerabilities": {
                "total_found": total_vulnerabilities,
                "requires_review": total_vulnerabilities > 0
            },
            "timestamp": datetime.now().isoformat(),
            "enhancements_used": {
                "tool_availability_check": True,
                "parallel_execution": True,
                "result_caching": enable_cache,
                "error_retry": enable_retry,
                "tool_fallback": enable_fallback
            }
        }
        
        logger.info(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ âœ… SCAN COMPLETED                                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Successful:      {len(successful_tools)}/{len(tasks):<44} â•‘
â•‘ Failed:          {len(failed_tools):<44} â•‘
â•‘ Cached:          {cached_results:<44} â•‘
â•‘ Vulnerabilities: {total_vulnerabilities:<44} â•‘
â•‘ Total Time:      {total_time:.2f}s{'':<38} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        
        return jsonify(response)
    
    except Exception as e:
        logger.error(f"âŒ Enhanced smart scan error: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


@intelligence_enhanced_bp.route("/cache-stats", methods=["GET"])
def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from core.cache.scan_cache import scan_cache
        
        stats = scan_cache.get_stats()
        
        return jsonify({
            "success": True,
            "cache_stats": stats,
            "timestamp": datetime.now().isoformat()
        })
    
    except Exception as e:
        logger.error(f"âŒ Cache stats error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@intelligence_enhanced_bp.route("/cache-clear", methods=["POST"])
def clear_cache():
    """æ¸…é™¤ç¼“å­˜"""
    try:
        from core.cache.scan_cache import scan_cache
        
        data = request.get_json() or {}
        pattern = data.get('pattern')
        
        count = scan_cache.clear_all(pattern)
        
        return jsonify({
            "success": True,
            "cleared_entries": count,
            "message": f"Cleared {count} cache entries",
            "timestamp": datetime.now().isoformat()
        })
    
    except Exception as e:
        logger.error(f"âŒ Cache clear error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500
